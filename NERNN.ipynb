{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERNN: Named Entity Recognition with Word Embeddings and Char RNNs\n",
    "- Use a combination of word embeddings and character embeddings + RNN to predict whether a word is a named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Computational imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Keras imports\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Random Data Generation\n",
    "1. Gen random sentences\n",
    "2. Construct a list of word inputs as well as a list of character inputs\n",
    "3. Pad the word and character inputs as well as the tags\n",
    "4. Find end of word indices in order to index into the character inputs (or the output representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct random sentences\n",
    "import random\n",
    "\n",
    "s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "\n",
    "def sing_sen_maker():\n",
    "    '''Makes a random senctence from the different parts of speech. Uses a SINGULAR subject'''\n",
    "    return random.choice(s_nouns), random.choice(s_verbs), random.choice(s_nouns).lower() or random.choice(p_nouns).lower(), random.choice(infinitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define random data generator\n",
    "def generate_data(num_examples, word_vocab_size, char_vocab_size):\n",
    "    # Word sequence \n",
    "    sentences = []\n",
    "    sent_lens = []\n",
    "\n",
    "    # Character sequences\n",
    "    word_lens = []  # for indexing after \n",
    "    chars_lens = []  # length of example in characters\n",
    "\n",
    "    # tags\n",
    "    tags = []\n",
    "\n",
    "    # Generate sentences\n",
    "    for _ in range(num_examples):\n",
    "        # new sentence\n",
    "        sent = ' '.join(sing_sen_maker())\n",
    "        words = word_tokenize(sent)\n",
    "        sentences.append(words)\n",
    "\n",
    "        # track the length of the sentence\n",
    "        sent_len = len(words)\n",
    "        sent_lens.append(sent_len)\n",
    "\n",
    "        # track the lengths of the words\n",
    "        words_len = map(len, words)\n",
    "        word_lens.append(words_len)\n",
    "\n",
    "        # track the length of the document in characters\n",
    "        char_len = sum(words_len) + sent_len - 1\n",
    "        chars_lens.append(char_len)\n",
    "\n",
    "        tag = (np.random.rand(sent_len) <= 0.2).astype('int')\n",
    "        tags.append(tag)\n",
    "\n",
    "    # ==================================== #\n",
    "    # Vocablulary Construction and Padding #\n",
    "    # ==================================== #\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    # Function for vocab construction\n",
    "    def construct_map(element_lists, vocab_size):\n",
    "        '''\n",
    "        Constructs a vocabulary from \n",
    "        '''\n",
    "        c = Counter()\n",
    "        for els in element_lists:\n",
    "            c.update(els)\n",
    "\n",
    "        most_common = [x[0] for x in c.most_common(vocab_size)]\n",
    "        hash_map = dict(zip(most_common, range(1, len(most_common)+1)))\n",
    "\n",
    "        return hash_map\n",
    "\n",
    "    def encode_strings(element_lists, hash_map):\n",
    "        '''\n",
    "        Encode the element_list in terms of integers\n",
    "        NOTE: 0 is reserved for masking\n",
    "        '''\n",
    "        new_element_list = []\n",
    "        for els in element_lists:\n",
    "            new_els = map(lambda x: hash_map.get(x, len(hash_map)+1), els)\n",
    "            new_element_list.append(new_els)\n",
    "        return new_element_list\n",
    "\n",
    "    # ====================== #\n",
    "    # 1. First the sentences #\n",
    "    # ====================== #\n",
    "\n",
    "    # Encode the words as integers\n",
    "    word_map = construct_map(sentences, word_vocab_size)\n",
    "    sentences_enc = encode_strings(sentences, word_map)\n",
    "\n",
    "    # Pad the sentences\n",
    "    max_sent_len = max(sent_lens)\n",
    "    sentences_enc = sequence.pad_sequences(sentences_enc, maxlen=max_sent_len,\n",
    "                                           padding='post', value=0, dtype='int64')\n",
    "\n",
    "    # ========================= #\n",
    "    # 2. Second, the characters #\n",
    "    # ========================= #\n",
    "\n",
    "    char_lists = [list(' '.join(sent)) for sent in sentences]\n",
    "\n",
    "    # Encode the characters\n",
    "    char_map = construct_map(char_lists, vocab_size=char_vocab_size)\n",
    "    char_enc = encode_strings(char_lists, char_map)\n",
    "\n",
    "    # Pad the characters\n",
    "    max_char_len = max(chars_lens)\n",
    "    char_enc = sequence.pad_sequences(char_enc, maxlen=max_char_len,\n",
    "                                           padding='post', value=0, dtype='int64')\n",
    "\n",
    "    # ============ #\n",
    "    # Pad the tags #\n",
    "    # ============ #\n",
    "    tags_padded = sequence.pad_sequences(tags, maxlen=max_sent_len, \n",
    "                                                       padding='post', value=-1., dtype='int64')\n",
    "\n",
    "    # ================================= #\n",
    "    # Construct the end of word indices #\n",
    "    # ================================= #\n",
    "    # NOTE: This will be leading spaces through the end of the word\n",
    "\n",
    "    word_end_inds = []\n",
    "    for wl, cl in zip(word_lens, chars_lens):\n",
    "        w_inds = np.cumsum(wl) + np.arange(len(wl)) - 1\n",
    "        word_end_inds.append(w_inds)\n",
    "\n",
    "    # Also need to pad the end of word indices\n",
    "    word_end_inds = sequence.pad_sequences(word_end_inds, maxlen=max_sent_len, padding='post', value=-1, dtype='int64')\n",
    "    \n",
    "    return sentences_enc, char_enc, word_end_inds, max_sent_len, max_char_len, tags_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, GRU, Dense, Lambda, InputLayer, TimeDistributed, Layer, Input, merge, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Going to need a custom layer for selecting the end of words in the character RNN\n",
    "class GatherLayer(Layer):\n",
    "    '''\n",
    "    Scans over the batch to gather specific indices along the time axis\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GatherLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(GatherLayer, self).build(input_shape)\n",
    "        \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        '''\n",
    "        Compute the mask\n",
    "        '''\n",
    "        return K.cast(K.not_equal(x[1], -1), 'bool')\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        '''\n",
    "        First input is the rnn out (batch_size, max_word_steps, char_lstm_dim)\n",
    "        Second input is indicies to gather (batch_size, max_word_steps)\n",
    "        '''\n",
    "        rnn_inp = inputs[0]\n",
    "        ind_inp = inputs[1]\n",
    "        \n",
    "        ind_inp_zeroed = tf.select(tf.not_equal(ind_inp, -1), ind_inp, tf.zeros_like(ind_inp, dtype='int64'))\n",
    "        \n",
    "        def f(inp):\n",
    "            '''\n",
    "            Gathers the inds for the input mat of (max_char_len, char_lstm_dim)\n",
    "            '''\n",
    "            mat = inp[0]\n",
    "            inds = inp[1]\n",
    "            return tf.gather(mat, inds)\n",
    "        \n",
    "        map_fn_out = tf.map_fn(f, elems=(rnn_inp, ind_inp_zeroed), dtype='float32')\n",
    "        \n",
    "        return map_fn_out\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        rnn_shape = input_shape[0]\n",
    "        ind_shape = input_shape[1]\n",
    "        return (rnn_shape[0], ind_shape[1], rnn_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_model(word_vocab_size, char_vocab_size,\n",
    "                    w_emb_dim=100, w_lstm_dim=128, \n",
    "                    c_emb_dim=100, c_lstm_dim=128):\n",
    "\n",
    "    # ===================== #\n",
    "    # 1. Construct word RNN #\n",
    "    # ===================== #\n",
    "\n",
    "    word_model = Sequential()\n",
    "    word_model.add(Embedding(input_dim=word_vocab_size+2, output_dim=w_emb_dim, \n",
    "                             input_length=None, mask_zero=True, name='word_embedding'))\n",
    "\n",
    "#     word_model.add(Bidirectional(GRU(w_lstm_dim, return_sequences=True, dropout_W=0.5)))\n",
    "\n",
    "    # ============================== #\n",
    "    # 2. Construct the character RNN #\n",
    "    # ============================== #\n",
    "\n",
    "    char_model = Sequential()\n",
    "    char_model.add(Embedding(input_dim=char_vocab_size+2, output_dim=c_emb_dim, \n",
    "                             input_length=None, mask_zero=True, name='char_embedding'))\n",
    "    char_model.add(Bidirectional(GRU(c_lstm_dim, return_sequences=True)))\n",
    "    temp_out = char_model.output\n",
    "\n",
    "    # ====================================== #\n",
    "    # 3. Merge the Word RNN and the Char RNN #\n",
    "    # ====================================== #\n",
    "\n",
    "    # Create an input for the matrix of word end indices\n",
    "    inds = Input(shape=(None,), dtype='int64')\n",
    "    \n",
    "    # Slice the character model out\n",
    "    char_model_slice = GatherLayer()([temp_out, inds])\n",
    "\n",
    "    # Concatenate the outputs of the word model and the sliced character model\n",
    "    merge_out = merge([word_model.output, char_model_slice], mode='concat', concat_axis=2)\n",
    "\n",
    "    # Add Bidirectional lstm here\n",
    "    gru_out = Bidirectional(GRU(w_emb_dim, return_sequences=True, dropout_W=0.5))(merge_out)\n",
    "    \n",
    "    # =================================== #\n",
    "    # 4. Compute Output and Compile Model #\n",
    "    # =================================== #\n",
    "\n",
    "    # Time distribute a final layer for binary output\n",
    "    fout = TimeDistributed(Dense(1, activation='sigmoid'))(merge_out)\n",
    "    \n",
    "    model = Model([word_model.input, char_model.input, inds], output=[fout])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Train the Model on Fake Data to Test It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "num_examples = 1000\n",
    "word_vocab_size = 100\n",
    "char_vocab_size = 30\n",
    "\n",
    "sentences_enc, char_enc, word_end_inds, max_sent_len, max_char_len, tags_padded = generate_data(num_examples,\n",
    "                                                                                                word_vocab_size, \n",
    "                                                                                                char_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/areiner/tensorflow/lib/python2.7/site-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = construct_model(word_vocab_size, char_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 20s - loss: 0.5558 - acc: 0.4173    \n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 18s - loss: 0.5192 - acc: 0.4242    \n",
      "Seems to work!\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on fake data\n",
    "model.fit([sentences_enc, char_enc, word_end_inds], np.expand_dims(tags_padded, 2), nb_epoch=2)\n",
    "print \"Seems to work!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Bring in Wikipedia Data to Train On\n",
    "- Downloaded WikiNER data into data directory\n",
    "- Will read in pre-split train/test/dev data into pandas\n",
    "- Preprocess the data to only tag PER (peoples names)\n",
    "- Will batch sentences together into 'Documents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "def prepend_zeros(arr, num_zeros=1, dtype='int64'):\n",
    "    '''\n",
    "    Takes a 1-D numpy array and prepends the specified number of zeros\n",
    "    '''\n",
    "    zs = np.zeros(shape=(num_zeros,), dtype=dtype)\n",
    "    return np.concatenate([zs, arr], axis=0)\n",
    "\n",
    "def spans(txt, tokens):\n",
    "    '''\n",
    "    Takes the original (read: \"untokenized\" text) and the tokens and returns a list of word\n",
    "    end indices.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    txt : string\n",
    "        untokenized / raw string we want to index the tokens into\n",
    "    tokens : list, array\n",
    "        list of tokens that make up the txt\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    word_inds : list\n",
    "        list of word span indices for the tokens into the txt\n",
    "    '''\n",
    "    word_inds = []\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        word_inds.append((offset, offset+len(token)))\n",
    "        offset += len(token)\n",
    "    return word_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Methods for Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "\n",
    "# ==== #\n",
    "# Vars #\n",
    "# ==== #\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = './data'\n",
    "WIKI_DATA = os.path.join(DATA_DIR, 'WikiGold')\n",
    "\n",
    "# Misc.\n",
    "DOCSTART_TAG = '-DOCSTART-'\n",
    "\n",
    "# ======================================= #\n",
    "# Methods for Reading and Processing Data #\n",
    "# ======================================= #\n",
    "\n",
    "# Read in the data only (no processing except putting in in pandas)\n",
    "def read_wiki_datasets(data_dir):\n",
    "    '''\n",
    "    Reads in the wikipedia datasets from the data directory\n",
    "    \n",
    "    Paramters\n",
    "    =========\n",
    "    data_dir : str\n",
    "        path to the data directory\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    train_df : pandas.core.frame.DataFrame\n",
    "        dataframe with training data\n",
    "    test_df : pandas.core.frame.DataFrame\n",
    "        dataframe with test data\n",
    "    dev_df : pandas.core.frame.DataFrame\n",
    "        dataframe with dev data\n",
    "    TODO: (areiner) what the hell is dev data?... I didn't do these splits\n",
    "    '''\n",
    "    \n",
    "    dataset_names = ('train', 'test', 'dev')\n",
    "    datasets = []  # ordered train, test, dev\n",
    "    for dname in dataset_names: \n",
    "        found_datasets = glob.glob(data_dir + '/*' + dname + '*.pkl')\n",
    "        if len(found_datasets)==0:\n",
    "            print \"No dataset with name {} found\".format(dname)\n",
    "        elif len(found_datasets) > 1:\n",
    "            print \"Multiple dataset with name {} found\".format(dname)\n",
    "        else:\n",
    "            df = pd.read_pickle(found_datasets[0])\n",
    "            datasets.append(df)\n",
    "    return datasets\n",
    "\n",
    "# Method to filter all tags such that it is only 'O' or 'PER'\n",
    "def modify_wikigold_tags(df):\n",
    "    '''\n",
    "    Takes in a dataframe with columns \"sentence\" and \"tags\" and modifies\n",
    "    the \"tags\" column to convert everything that is not \"B-PER\" to \"O\" and\n",
    "    converts \"B-PER\" to \"PER\"\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        input dataframe of sentences and tags (with columns \"sentence\" and \"tags\")\n",
    "    '''\n",
    "    def modify_taglist(taglist):\n",
    "        '''\n",
    "        Function to apply to each array of tags in each cell of the \"tags\"\n",
    "        column in the dataframe\n",
    "        '''\n",
    "        m = {'B-PER': 'PER'}\n",
    "        return map(lambda v: m.get(v, 'O'), taglist)\n",
    "\n",
    "    df['tags'] = df['tags'].apply(modify_taglist)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Method to find the index for document separations\n",
    "def calc_docstart_inds(df):\n",
    "    '''\n",
    "    Takes in a dataframe and looks for specific document start tags\n",
    "    '''\n",
    "    return df.index[df.sentence.map(lambda x: DOCSTART_TAG in x)].get_values()\n",
    "\n",
    "# Method to concatenate sentence tags to document tags\n",
    "def construct_doc_tag(dfs, max_sent_per_doc=None):\n",
    "    '''\n",
    "    Takes a DataFrame slice and returns a dataframe of tokens and tags that\n",
    "    concatenates all sentence tokens and tags for the whole dataframe, potentially\n",
    "    into groups of size specified by max_sent_per_doc size\n",
    "    '''\n",
    "    new_df = pd.DataFrame(columns=['document', 'tags'])\n",
    "    \n",
    "    sentences = []\n",
    "    tags = []\n",
    "    \n",
    "    for i, (_, row) in enumerate(dfs.iterrows()):\n",
    "        \n",
    "        if (max_sent_per_doc is not None) and i!=0 and (i % max_sent_per_doc == 0):\n",
    "            new_df = new_df.append({'document': sentences, 'tags': tags}, ignore_index=True)\n",
    "            sentences = []\n",
    "            tags = []\n",
    "\n",
    "        tokens = row.sentence\n",
    "        tagseq = row.tags\n",
    "        \n",
    "        sentences.extend(tokens)\n",
    "        tags.extend(tagseq)\n",
    "    \n",
    "    new_df = new_df.append({'document': sentences, 'tags': tags}, ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Method to cluster sentence token dataframes into documents\n",
    "def transform_sent_to_docs(df, max_sent_per_doc=None):\n",
    "    '''\n",
    "    Transform a DataFrame of lists of tokens and tags per sentence into\n",
    "    a DataFrame of \"Documents\" and the tags for that document.\n",
    "    At a minimum, we split by document length as per the wikipedia page\n",
    "    '''\n",
    "    doc_df = pd.DataFrame(columns=['document', 'tags'])\n",
    "    \n",
    "    # 1. Calculate the indices of Document starts\n",
    "    doc_starts = prepend_zeros(calc_docstart_inds(df))\n",
    "    \n",
    "    # 2. Slice the df for the sentences in each document\n",
    "    for i, (start, end) in enumerate(zip(doc_starts[:-1], doc_starts[1:])):\n",
    "        if i != 0:\n",
    "            start += 1\n",
    "        dfs = df.iloc[start:end]\n",
    "        inc_doc_df = construct_doc_tag(dfs, max_sent_per_doc=max_sent_per_doc)\n",
    "        doc_df = doc_df.append(inc_doc_df)\n",
    "    \n",
    "    doc_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return doc_df\n",
    "        \n",
    "# Method that adds a column which contains the untokenized sentence / document\n",
    "def untokenize_column(df, col='document', new_col='document_string'):\n",
    "    '''\n",
    "    Untokenizes a column of lists of tokens. The intended use in the pipeline is to\n",
    "    construct untokenized strings after clustering sentences into documents or partial\n",
    "    documents.\n",
    "    '''\n",
    "    df[new_col] = df[col].map(untokenize)\n",
    "    return df\n",
    "\n",
    "# Method that adds a column which contains the index of the tokens into the untokenized sentence / document\n",
    "def word_index_columns(df, doc_col='document_string', tok_col='document'):\n",
    "    '''\n",
    "    Takes a column of strings (either sentences or documents as long as it's one continuous string) \n",
    "    and creats two new columns a 'word_start_inds' column that contains the start indices of all tokens\n",
    "    and a 'word_end_inds' that contains the end indices of all tokens\n",
    "    '''\n",
    "    # Calculate the word spanning indices\n",
    "    word_inds = df.apply(lambda r: spans(r[doc_col], r[tok_col]), axis=1)\n",
    "    word_start_inds = word_inds.apply(lambda v: [e[0] for e in v])\n",
    "    word_end_inds = word_inds.apply(lambda v: [e[1] - 1 for e in v])\n",
    "    \n",
    "    # Insert new columns\n",
    "    df['word_start_inds'] = word_start_inds\n",
    "    df['word_end_inds'] = word_end_inds\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Methods for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct word and character maps\n",
    "def construct_map(element_lists, vocab_size=None):\n",
    "    '''\n",
    "    Constructs a vocabulary from \n",
    "    '''\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for els in element_lists:\n",
    "        c.update(els)\n",
    "\n",
    "    if vocab_size is not None:\n",
    "        most_common = [x[0] for x in c.most_common(vocab_size)]\n",
    "        hash_map = dict(zip(most_common, range(1, len(most_common)+1)))\n",
    "    else:\n",
    "        hash_map = dict(zip(c.keys(), range(1, len(c)+1)))\n",
    "\n",
    "    return hash_map\n",
    "\n",
    "def reverse_map(m):\n",
    "    m_inv = dict(((ind, k) for k, v in m.iteritems()))\n",
    "    return m_inv\n",
    "    \n",
    "def character_column(df):\n",
    "    '''\n",
    "    Inserts a column of the individual characters into the dataframe \n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        dataframe containing the string / untokenized documents\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    new_df : pandas.core.frame.DataFrame\n",
    "        Updated df\n",
    "    '''\n",
    "    df['chars'] = df['document_string'].map(lambda v: list(v))\n",
    "    return df\n",
    "\n",
    "def construct_word_char_maps(df, vocab_size=None, return_inv_dicts=False):\n",
    "    '''\n",
    "    Construct the word and character maps from the dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    df : pandas DataFrame\n",
    "        dataframe with tokens and character lists already constructed\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    token_map : dict\n",
    "    char_map : dict\n",
    "    '''\n",
    "    token_map = construct_map(df['document'])\n",
    "    char_map = construct_map(df['chars'])\n",
    "    \n",
    "    if return_inv_dicts:\n",
    "        token_map_inv = reverse_map(token_map)\n",
    "        char_map_inv = reverse_map(char_map)\n",
    "        \n",
    "        return token_map, char_map, token_map_inv, char_map_inv\n",
    "    else:\n",
    "        return token_map, char_map\n",
    "\n",
    "def encode_strings(element_lists, hash_map):\n",
    "    '''\n",
    "    Encode the element_list in terms of integers\n",
    "    NOTE: 0 is reserved for masking\n",
    "    '''\n",
    "    new_element_list = []\n",
    "    for els in element_lists:\n",
    "        new_els = map(lambda x: hash_map.get(x, len(hash_map)+1), els)\n",
    "        new_element_list.append(new_els)\n",
    "    return new_element_list\n",
    "    \n",
    "def encode_tokens_chars(df, token_map, char_map):\n",
    "    '''\n",
    "    Encode the tokens and characters\n",
    "    '''\n",
    "    df['token_enc'] = encode_strings(df['document'], token_map)\n",
    "    df['char_enc'] = encode_strings(df['chars'], char_map)\n",
    "    return df\n",
    "\n",
    "def _encode_tags(tag_list):\n",
    "    '''\n",
    "    Encode a taglist\n",
    "    '''\n",
    "    def _tag_str_to_int(tag_str):\n",
    "        if tag_str == 'PER':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    tags_enc = map(_tag_str_to_int, tag_list)\n",
    "    return tags_enc\n",
    "    \n",
    "def encode_tags(df):\n",
    "    '''\n",
    "    Encode the tags as binary outcomes\n",
    "    \n",
    "    Parameters\n",
    "    =========\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        dataframe with character tags that are in the set {'O', and 'PER'}\n",
    "    '''\n",
    "    \n",
    "    df['tags_enc'] = df['tags'].apply(_encode_tags)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Full data processing pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_wikigold_dataset(df, max_sent_per_doc=None):\n",
    "    \n",
    "    # Text Processing\n",
    "    df = modify_wikigold_tags(df)\n",
    "    df = transform_sent_to_docs(df, max_sent_per_doc=max_sent_per_doc) # Concatenate sentence tokens into topics\n",
    "    df = untokenize_column(df)  # (approximately) concatenate the tokens into documents\n",
    "    df = word_index_columns(df)  # index the tokens into the untokenized strings\n",
    "    df = character_column(df)\n",
    "\n",
    "    # Encoding work\n",
    "    tm, cm = construct_word_char_maps(df)  # first construct the hash maps\n",
    "    df = encode_tokens_chars(df, tm, cm)  # encode the tokens and the characters\n",
    "    df = encode_tags(df)  # encode the tags as binary outcomes 'O' -> 0 and 'PER' -> 1\n",
    "    \n",
    "    return df, tm, cm\n",
    "\n",
    "def run_wikigold_data_pipeline(data_path, max_sent_per_doc=None):\n",
    "    '''\n",
    "    Runs the entire transformation pipeline for WikiGold data.\n",
    "    \n",
    "    Pipeline Steps\n",
    "    --------------\n",
    "    1. Read in wiki datasets (train, test, dev)\n",
    "    2. Modify the IOB tags to only 'O' and 'PER'\n",
    "    For each dataset in the wiki datasets:\n",
    "        3. Take sentence examples and concatenate them into individual documents\n",
    "        4. Take the documents which consist of lists of tokens and \"untokenize\" them into continuous strings\n",
    "        5. Take the tokens and index them into the untokenized text so we have start and end indices for each token\n",
    "        6. Split up the untokenized text into a list of characters\n",
    "        7. Construct element -> monotonically increasing index map for both tokens and characters\n",
    "        8. Encode the tokens and characters by their index in their respective maps\n",
    "        9. Encode the tags as binary outcomes 'O' -> 0 and 'PER' -> 1\n",
    "    '''\n",
    "    # 1. Reading in data\n",
    "    wiki_datasets = read_wiki_datasets(WIKI_DATA)\n",
    "\n",
    "    wiki_datasets_processed = []\n",
    "    wiki_datasets_maps = []\n",
    "    # 2. Run processing on each dataframe\n",
    "    for df in wiki_datasets:\n",
    "        new_df, token_map, char_map = process_wikigold_dataset(df, max_sent_per_doc=max_sent_per_doc)\n",
    "        wiki_datasets_processed.append(new_df)\n",
    "        wiki_datasets_maps.append((token_map, char_map))\n",
    "    \n",
    "    return wiki_datasets_processed, wiki_datasets_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train, test, dev), ((tm_train, cm_train), (tm_test, cm_test), (tm_dev, cm_dev)) = \\\n",
    "                                run_wikigold_data_pipeline(WIKI_DATA, max_sent_per_doc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Final steps - convert to numpy and pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_encoding_column(col, value=0):\n",
    "    '''\n",
    "    Takes a pandas series of lists of encodings and returns a single matrix with padded results\n",
    "    '''    \n",
    "    col_padded = sequence.pad_sequences(col.tolist(), padding='post', value=value)\n",
    "    return col_padded\n",
    "\n",
    "def generate_padded_data(df):\n",
    "    '''\n",
    "    Take the dataframe and return numpy matrices with padded encodings\n",
    "    '''\n",
    "    cols_to_pad = ['token_enc', 'char_enc', 'tags_enc', 'word_start_inds', 'word_end_inds']\n",
    "    padded_data = []\n",
    "    for col in cols_to_pad:\n",
    "        if ('tag' in col) or ('inds' in col):\n",
    "            col_pad = pad_encoding_column(df[col], value=-1)\n",
    "        else:\n",
    "            col_pad = pad_encoding_column(df[col])\n",
    "        padded_data.append(col_pad)\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Construct Model and Bring in Pre-trained word emebddings\n",
    "- For now, we will use GoogleNews embedding vectors with dimensionality 300\n",
    "- We will not pre-train character emebeddings for now\n",
    "    - Why? bc I haven't found character embedings yet and I would imagine the best representations can different markedly from use case to use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Construct a gensim model that we can use to access underlying embeddings from the massive GoogleNews embedding matrix\n",
    "- 3 Million words in GoogleNews Vocab with 300 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the gensim word2vec model\n",
    "w2vmodel = Word2Vec.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Construct the model and set the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/areiner/tensorflow/lib/python2.7/site-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "# Model construction\n",
    "word_vocab_size = len(tm_train)\n",
    "char_vocab_size = len(cm_train)\n",
    "# model = construct_model(word_vocab_size=word_vocab_size, char_vocab_size=char_vocab_size,\n",
    "#                         w_emb_dim=w2vmodel.vector_size)\n",
    "model = construct_model(word_vocab_size=word_vocab_size, char_vocab_size=char_vocab_size,\n",
    "                        w_emb_dim=100, w_lstm_dim=64, c_emb_dim=50, c_lstm_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a method for setting the models word_embedding layer to have pretrained embeddings\n",
    "def set_embeddings(w2v_model, keras_model, token_map):\n",
    "    '''\n",
    "    Takes in a gensim Word2Vec model and our keras model and then adapts the word_embeddings\n",
    "    in our model to the pre-trained vectors from the w2v model if they exist. Otherwise, they are\n",
    "    left to the original initalization\n",
    "    \n",
    "    Paramters\n",
    "    =========\n",
    "    w2v_model : gensim.models.word2vec.Word2Vec\n",
    "        Word2Vec model that is already loaded with pre-trained embedings\n",
    "    keras_model : keras.engine.training.Model\n",
    "        Keras model that has been constructed such that the word embedding layer has the same\n",
    "        number of embedding dimensions as the pre-trained embeddings\n",
    "    token_map : dict\n",
    "        map from token to index in the vocab\n",
    "    '''\n",
    "    word_emb_layer = keras_model.get_layer('word_embedding')\n",
    "    weights = word_emb_layer.get_weights()[0]\n",
    "    \n",
    "    for token, ind in token_map.iteritems():\n",
    "        try:\n",
    "            pre_trained_emb = w2v_model[token]\n",
    "            \n",
    "        except:\n",
    "            pre_trained_emb = weights[ind]\n",
    "        weights[ind] = pre_trained_emb\n",
    "    word_emb_layer.set_weights([weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the embeddings\n",
    "set_embeddings(w2vmodel, model, tm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Train the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "token_enc_train, char_enc_train, tags_enc_train, word_start_ind_train, word_end_ind_train = \\\n",
    "                                                                                        generate_padded_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.fit([token_enc_train, char_enc_train, word_end_ind_train], np.expand_dims(tags_enc_train, 2), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 12\n",
    "x_in = [token_enc_train[i:i+1], char_enc_train[i:i+1], word_end_ind_train[i:i+1]]\n",
    "\n",
    "out = model.predict(x_in)[0, :, 0]\n",
    "\n",
    "token_ex = np.array(train.document[i])\n",
    "tags_ex = np.array(train.tags[i])\n",
    "tags_enc_ex = np.array(train.tags_enc[i])\n",
    "\n",
    "m = token_ex[(tags_enc_ex.nonzero()[0])]\n",
    "\n",
    "print m\n",
    "print zip(m, out[tags_enc_ex.nonzero()[0]])\n",
    "out_sorted = out.argsort()\n",
    "out_sorted = out_sorted[out_sorted<len(token_ex)]\n",
    "print \"Lowest scores:\", token_ex[out_sorted[:30]]\n",
    "print \"Highest scores:\", token_ex[out_sorted[-20:]]\n",
    "print \"Highest scores:\", out[out_sorted[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "token_enc_test, char_enc_test, tags_enc_test, word_start_ind_test, word_end_ind_test = \\\n",
    "                                                                                        generate_padded_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dorothea' 'Dorothea' 'von' 'Schlegel' 'Rahel' 'Levin' 'Henriette' 'Herz'\n",
      " 'Madame' 'de' 'Sta\\xc3\\xab' 'Friedrich' 'Philipp' 'Moses' 'Mendelssohn'\n",
      " 'Immanuel' 'Kant' 'John' 'Locke' 'Alexander' 'Pope' 'Dorothea']\n",
      "[('Dorothea', 0.00082105485), ('Dorothea', 0.0094426926), ('von', 0.0055673928), ('Schlegel', 0.0051288288), ('Rahel', 0.00068829377), ('Levin', 9.7050888e-06), ('Henriette', 0.00013976262), ('Herz', 2.7702123e-05), ('Madame', 0.010900004), ('de', 0.0044732802), ('Sta\\xc3\\xab', 0.0014611182), ('Friedrich', 0.52245504), ('Philipp', 0.00066411082), ('Moses', 0.00057833287), ('Mendelssohn', 0.0017986018), ('Immanuel', 0.00022611055), ('Kant', 9.4189062e-07), ('John', 7.342115e-05), ('Locke', 7.0163347e-05), ('Alexander', 0.0013022374), ('Pope', 0.051861312), ('Dorothea', 0.01213771)]\n",
      "Lowest scores: ['Kant' 'and' 'and' 'adopted' 'and' 'greatest' 'and' 'critics' 'of'\n",
      " 'translator' 'Levin' ',' 'novelists' 'to' 'musicians' ',' 'convert'\n",
      " 'leading' ',' 'Herz' ',' ',' 'as' ',' 'medieval' 'of' 'which' 'of'\n",
      " 'daughter' 'name']\n",
      "Highest scores: ['who' 'hymns' 'died' 'throughout' '1829' '1839' 'her' 'Friedrich' 'death'\n",
      " 'surrounded' 'moved' ')' '17th' ')' ')' 'Frankfurt' 'surrounded' 'until'\n",
      " 'in' 'in']\n",
      "Highest scores: [ 0.0754  0.076   0.0788  0.0841  0.1002  0.1514  0.3697  0.5225  0.6907\n",
      "  0.7128  0.808   0.8208  0.8746  0.9371  0.9451  0.9731  0.9801  0.9969\n",
      "  0.9973  0.9979]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "x_in = [token_enc_test[i:i+1], char_enc_test[i:i+1], word_end_ind_test[i:i+1]]\n",
    "\n",
    "out = model.predict(x_in)[0, :, 0]\n",
    "\n",
    "token_ex = np.array(test.document[i])\n",
    "tags_ex = np.array(test.tags[i])\n",
    "tags_enc_ex = np.array(test.tags_enc[i])\n",
    "\n",
    "m = token_ex[(tags_enc_ex.nonzero()[0])]\n",
    "\n",
    "print m\n",
    "print zip(m, out[tags_enc_ex.nonzero()[0]])\n",
    "out_sorted = out.argsort()\n",
    "out_sorted = out_sorted[out_sorted<len(token_ex)]\n",
    "print \"Lowest scores:\", token_ex[out_sorted[:30]]\n",
    "print \"Highest scores:\", token_ex[out_sorted[-20:]]\n",
    "print \"Highest scores:\", out[out_sorted[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.30020115e-04,   8.30020115e-04,   8.30020115e-04,\n",
       "         8.30020115e-04,   8.30020115e-04,   8.30020115e-04,\n",
       "         8.30020115e-04,   8.30020115e-04,   8.30020115e-04,\n",
       "         8.30020115e-04,   8.30020115e-04,   8.30020115e-04,\n",
       "         8.30020115e-04,   8.30020115e-04,   8.30020115e-04,\n",
       "         8.30020115e-04,   8.30020115e-04,   8.30020115e-04,\n",
       "         8.30020115e-04,   8.30020115e-04,   8.30020115e-04,\n",
       "         8.30020115e-04,   8.30020115e-04,   9.63850296e-04,\n",
       "         1.43575238e-03,   1.99742848e-03,   9.65667307e-01,\n",
       "         9.86828208e-01,   9.99197304e-01,   9.99897480e-01], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[out_sorted[-30:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17, 18, 19]),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0] >= 0.5)[:, 0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.00019067]\n",
      "1 [ 0.00043793]\n",
      "2 [  6.12061240e-06]\n",
      "3 [  1.58033617e-05]\n",
      "4 [  4.24458989e-07]\n",
      "5 [ 0.00011363]\n",
      "6 [ 0.00010162]\n",
      "7 [  1.08037614e-06]\n",
      "8 [  1.75458936e-05]\n",
      "9 [  4.20558354e-05]\n",
      "10 [  4.43179033e-07]\n",
      "11 [  6.54117741e-07]\n",
      "12 [  9.87347266e-06]\n",
      "13 [  1.81310043e-06]\n",
      "14 [  1.18962475e-06]\n",
      "15 [ 0.00196883]\n",
      "16 [  3.62454812e-05]\n",
      "17 [ 0.99313885]\n",
      "18 [ 0.99983346]\n",
      "19 [ 0.97283983]\n",
      "20 [ 0.00040712]\n",
      "21 [  2.31974832e-06]\n",
      "22 [  2.13084841e-05]\n",
      "23 [ 0.00026095]\n",
      "24 [  2.85764304e-06]\n",
      "25 [  6.44781903e-06]\n",
      "26 [  7.43022756e-05]\n",
      "27 [  1.64395004e-07]\n",
      "28 [  1.74500087e-06]\n",
      "29 [  1.41274072e-07]\n",
      "30 [  7.09953483e-08]\n",
      "31 [  2.35899483e-06]\n",
      "32 [  2.13374938e-06]\n",
      "33 [  2.04589123e-05]\n",
      "34 [  6.47321883e-07]\n",
      "35 [  3.27852149e-06]\n",
      "36 [  7.01268291e-05]\n",
      "37 [  3.58893194e-05]\n",
      "38 [  4.99607097e-07]\n",
      "39 [  1.68278433e-07]\n",
      "40 [  5.87318937e-06]\n",
      "41 [  4.08215328e-07]\n",
      "42 [  9.65235245e-07]\n",
      "43 [  1.19888762e-06]\n",
      "44 [  4.67477621e-05]\n",
      "45 [  5.74571686e-07]\n",
      "46 [  6.05322566e-05]\n",
      "47 [  4.70991154e-06]\n",
      "48 [  7.84170988e-05]\n",
      "49 [  3.09550330e-07]\n",
      "50 [  1.23511381e-06]\n",
      "51 [  5.78260256e-07]\n",
      "52 [  1.30329482e-07]\n",
      "53 [  3.48386420e-05]\n",
      "54 [  7.33479965e-06]\n",
      "55 [  1.60168781e-06]\n",
      "56 [  1.10234211e-07]\n",
      "57 [  4.20374317e-06]\n",
      "58 [  8.53462154e-07]\n",
      "59 [  1.46018493e-07]\n",
      "60 [  1.44129735e-05]\n",
      "61 [  3.10259893e-05]\n",
      "62 [  1.39281283e-05]\n",
      "63 [  3.69722839e-05]\n",
      "64 [ 0.00013662]\n",
      "65 [ 0.00010644]\n",
      "66 [  9.29057421e-07]\n",
      "67 [  1.41475284e-05]\n",
      "68 [  4.71534008e-07]\n",
      "69 [  1.16312776e-05]\n",
      "70 [  6.78237370e-07]\n",
      "71 [  6.02063592e-05]\n",
      "72 [  9.01290841e-05]\n",
      "73 [  5.52506826e-05]\n",
      "74 [  1.53603457e-06]\n",
      "75 [  1.95485609e-05]\n",
      "76 [  1.55323414e-06]\n",
      "77 [  5.74370063e-07]\n",
      "78 [ 0.0002851]\n",
      "79 [  9.45266834e-07]\n",
      "80 [  1.46977470e-06]\n",
      "81 [  1.83903066e-05]\n",
      "82 [  2.52458108e-06]\n",
      "83 [  3.54110784e-06]\n",
      "84 [  2.83515528e-05]\n",
      "85 [  1.14439554e-05]\n",
      "86 [  3.38239806e-05]\n",
      "87 [  1.21431833e-06]\n",
      "88 [  2.32533421e-06]\n",
      "89 [  4.81881307e-06]\n",
      "90 [  6.72390286e-07]\n",
      "91 [  9.47350145e-08]\n",
      "92 [  3.25503152e-06]\n",
      "93 [  1.22836468e-07]\n",
      "94 [  2.70209426e-07]\n",
      "95 [  1.11280322e-07]\n",
      "96 [  3.02178000e-06]\n",
      "97 [  1.57617947e-07]\n",
      "98 [  2.23517218e-05]\n",
      "99 [  5.00502319e-05]\n",
      "100 [  1.09207133e-06]\n",
      "101 [  9.72890120e-05]\n",
      "102 [ 0.00024362]\n",
      "103 [  2.24512246e-06]\n",
      "104 [  7.90863705e-05]\n",
      "105 [ 0.00012532]\n",
      "106 [  3.46265864e-07]\n",
      "107 [  1.73455973e-07]\n",
      "108 [  6.43537896e-06]\n",
      "109 [  3.76718162e-05]\n",
      "110 [  1.06173434e-06]\n",
      "111 [  1.86415764e-06]\n",
      "112 [  9.44736996e-07]\n",
      "113 [  1.12382804e-06]\n",
      "114 [  2.17105826e-05]\n",
      "115 [  2.09237811e-07]\n",
      "116 [  1.28887211e-06]\n",
      "117 [  4.26351136e-07]\n",
      "118 [  5.33580896e-06]\n",
      "119 [  2.66940418e-07]\n",
      "120 [  8.30078264e-08]\n",
      "121 [  3.69730697e-06]\n",
      "122 [  1.43542607e-07]\n",
      "123 [  6.88840544e-07]\n",
      "124 [  4.53530902e-06]\n",
      "125 [  1.37081599e-06]\n",
      "126 [  3.48390422e-05]\n",
      "127 [  2.87912229e-07]\n",
      "128 [  2.35221360e-05]\n",
      "129 [  4.65574487e-07]\n",
      "130 [  6.20379069e-07]\n",
      "131 [  5.49047691e-05]\n",
      "132 [  3.11868607e-05]\n",
      "133 [ 0.00025155]\n",
      "134 [ 0.00025155]\n",
      "135 [ 0.00025155]\n",
      "136 [ 0.00025155]\n",
      "137 [ 0.00025155]\n",
      "138 [ 0.00025155]\n",
      "139 [ 0.00025155]\n",
      "140 [ 0.00025155]\n",
      "141 [ 0.00025155]\n",
      "142 [ 0.00025155]\n",
      "143 [ 0.00025155]\n",
      "144 [ 0.00025155]\n",
      "145 [ 0.00025155]\n",
      "146 [ 0.00025155]\n",
      "147 [ 0.00025155]\n",
      "148 [ 0.00025155]\n",
      "149 [ 0.00025155]\n",
      "150 [ 0.00025155]\n",
      "151 [ 0.00025155]\n",
      "152 [ 0.00025155]\n",
      "153 [ 0.00025155]\n",
      "154 [ 0.00025155]\n",
      "155 [ 0.00025155]\n",
      "156 [ 0.00025155]\n",
      "157 [ 0.00025155]\n",
      "158 [ 0.00025155]\n",
      "159 [ 0.00025155]\n",
      "160 [ 0.00025155]\n",
      "161 [ 0.00025155]\n",
      "162 [ 0.00025155]\n",
      "163 [ 0.00025155]\n",
      "164 [ 0.00025155]\n",
      "165 [ 0.00025155]\n",
      "166 [ 0.00025155]\n",
      "167 [ 0.00025155]\n",
      "168 [ 0.00025155]\n",
      "169 [ 0.00025155]\n",
      "170 [ 0.00025155]\n",
      "171 [ 0.00025155]\n",
      "172 [ 0.00025155]\n",
      "173 [ 0.00025155]\n",
      "174 [ 0.00025155]\n",
      "175 [ 0.00025155]\n",
      "176 [ 0.00025155]\n",
      "177 [ 0.00025155]\n",
      "178 [ 0.00025155]\n",
      "179 [ 0.00025155]\n",
      "180 [ 0.00025155]\n",
      "181 [ 0.00025155]\n",
      "182 [ 0.00025155]\n",
      "183 [ 0.00025155]\n",
      "184 [ 0.00025155]\n",
      "185 [ 0.00025155]\n",
      "186 [ 0.00025155]\n",
      "187 [ 0.00025155]\n",
      "188 [ 0.00025155]\n",
      "189 [ 0.00025155]\n",
      "190 [ 0.00025155]\n",
      "191 [ 0.00025155]\n",
      "192 [ 0.00025155]\n",
      "193 [ 0.00025155]\n",
      "194 [ 0.00025155]\n",
      "195 [ 0.00025155]\n",
      "196 [ 0.00025155]\n",
      "197 [ 0.00025155]\n",
      "198 [ 0.00025155]\n",
      "199 [ 0.00025155]\n",
      "200 [ 0.00025155]\n",
      "201 [ 0.00025155]\n",
      "202 [ 0.00025155]\n",
      "203 [ 0.00025155]\n",
      "204 [ 0.00025155]\n",
      "205 [ 0.00025155]\n",
      "206 [ 0.00025155]\n",
      "207 [ 0.00025155]\n",
      "208 [ 0.00025155]\n",
      "209 [ 0.00025155]\n",
      "210 [ 0.00025155]\n",
      "211 [ 0.00025155]\n",
      "212 [ 0.00025155]\n",
      "213 [ 0.00025155]\n",
      "214 [ 0.00025155]\n",
      "215 [ 0.00025155]\n",
      "216 [ 0.00025155]\n",
      "217 [ 0.00025155]\n",
      "218 [ 0.00025155]\n",
      "219 [ 0.00025155]\n",
      "220 [ 0.00025155]\n",
      "221 [ 0.00025155]\n",
      "222 [ 0.00025155]\n",
      "223 [ 0.00025155]\n",
      "224 [ 0.00025155]\n",
      "225 [ 0.00025155]\n",
      "226 [ 0.00025155]\n",
      "227 [ 0.00025155]\n",
      "228 [ 0.00025155]\n",
      "229 [ 0.00025155]\n",
      "230 [ 0.00025155]\n",
      "231 [ 0.00025155]\n",
      "232 [ 0.00025155]\n",
      "233 [ 0.00025155]\n",
      "234 [ 0.00025155]\n",
      "235 [ 0.00025155]\n",
      "236 [ 0.00025155]\n",
      "237 [ 0.00025155]\n",
      "238 [ 0.00025155]\n",
      "239 [ 0.00025155]\n",
      "240 [ 0.00025155]\n",
      "241 [ 0.00025155]\n",
      "242 [ 0.00025155]\n",
      "243 [ 0.00025155]\n",
      "244 [ 0.00025155]\n",
      "245 [ 0.00025155]\n",
      "246 [ 0.00025155]\n",
      "247 [ 0.00025155]\n",
      "248 [ 0.00025155]\n",
      "249 [ 0.00025155]\n",
      "250 [ 0.00025155]\n",
      "251 [ 0.00025155]\n",
      "252 [ 0.00025155]\n",
      "253 [ 0.00025155]\n",
      "254 [ 0.00025155]\n",
      "255 [ 0.00025155]\n",
      "256 [ 0.00025155]\n",
      "257 [ 0.00025155]\n",
      "258 [ 0.00025155]\n",
      "259 [ 0.00025155]\n",
      "260 [ 0.00025155]\n",
      "261 [ 0.00025155]\n",
      "262 [ 0.00025155]\n",
      "263 [ 0.00025155]\n",
      "264 [ 0.00025155]\n",
      "265 [ 0.00025155]\n",
      "266 [ 0.00025155]\n",
      "267 [ 0.00025155]\n",
      "268 [ 0.00025155]\n",
      "269 [ 0.00025155]\n",
      "270 [ 0.00025155]\n",
      "271 [ 0.00025155]\n",
      "272 [ 0.00025155]\n",
      "273 [ 0.00025155]\n",
      "274 [ 0.00025155]\n",
      "275 [ 0.00025155]\n",
      "276 [ 0.00025155]\n",
      "277 [ 0.00025155]\n",
      "278 [ 0.00025155]\n",
      "279 [ 0.00025155]\n",
      "280 [ 0.00025155]\n",
      "281 [ 0.00025155]\n",
      "282 [ 0.00025155]\n",
      "283 [ 0.00025155]\n",
      "284 [ 0.00025155]\n",
      "285 [ 0.00025155]\n",
      "286 [ 0.00025155]\n",
      "287 [ 0.00025155]\n",
      "288 [ 0.00025155]\n",
      "289 [ 0.00025155]\n",
      "290 [ 0.00025155]\n",
      "291 [ 0.00025155]\n",
      "292 [ 0.00025155]\n",
      "293 [ 0.00025155]\n",
      "294 [ 0.00025155]\n",
      "295 [ 0.00025155]\n",
      "296 [ 0.00025155]\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(out[0]):\n",
    "    print i, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: How can we determine the end of word indices from the tokens?\n",
    "- we need to use the provided tokens and reconstruct the original string\n",
    "    - We do this via an \"untokenize\" function found off the shelf online\n",
    "    - If we tokenize again (using nltk word_tokenize) do we get the same result?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``\n",
      "\"\n",
      "bonus tracks: \" Battle of\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bonus tracks:\" Battle of One\"( an original song that was also set'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print word_tokenize(res.iloc[4].document_string)[257]\n",
    "print res.iloc[4].document[257]\n",
    "print untokenize(res.iloc[4].document[254:260])\n",
    "\n",
    "def untokenize2(tokens):\n",
    "    import string\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "untokenize2(res.iloc[4].document[254:270])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_kernel",
   "language": "python",
   "name": "tensorflow_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
