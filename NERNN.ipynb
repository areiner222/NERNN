{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERNN: Named Entity Recognition with Word Embeddings and Char RNNs\n",
    "- Use a combination of word embeddings and character embeddings + RNN to predict whether a word is a named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import gzip\n",
    "import glob\n",
    "\n",
    "# Computational imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Keras imports\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, GRU, Dense, Lambda, InputLayer, TimeDistributed, Layer, Input, merge, Bidirectional\n",
    "\n",
    "# gensim imports\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Defining the Keras model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Going to need a custom layer for selecting the end of words in the character RNN\n",
    "class GatherLayer(Layer):\n",
    "    '''\n",
    "    Scans over the batch to gather specific indices along the time axis\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GatherLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(GatherLayer, self).build(input_shape)\n",
    "        \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        '''\n",
    "        Compute the mask\n",
    "        '''\n",
    "        return K.cast(K.not_equal(x[1], -1), 'bool')\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        '''\n",
    "        First input is the rnn out (batch_size, max_word_steps, char_lstm_dim)\n",
    "        Second input is indicies to gather (batch_size, max_word_steps)\n",
    "        '''\n",
    "        rnn_inp = inputs[0]\n",
    "        ind_inp = inputs[1]\n",
    "        \n",
    "        ind_inp_zeroed = tf.select(tf.not_equal(ind_inp, -1), ind_inp, tf.zeros_like(ind_inp, dtype='int64'))\n",
    "        \n",
    "        def f(inp):\n",
    "            '''\n",
    "            Gathers the inds for the input mat of (max_char_len, char_lstm_dim)\n",
    "            '''\n",
    "            mat = inp[0]\n",
    "            inds = inp[1]\n",
    "            return tf.gather(mat, inds)\n",
    "        \n",
    "        map_fn_out = tf.map_fn(f, elems=(rnn_inp, ind_inp_zeroed), dtype='float32')\n",
    "        \n",
    "        return map_fn_out\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        rnn_shape = input_shape[0]\n",
    "        ind_shape = input_shape[1]\n",
    "        return (rnn_shape[0], ind_shape[1], rnn_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SegmentLayer(Layer):\n",
    "    '''\n",
    "    Takes a segmented sum\n",
    "    '''\n",
    "    def __init__(self, seg_func_name='sum', **kwargs):\n",
    "        super(SegmentLayer, self).__init__(**kwargs)\n",
    "        if seg_func_name == 'sum':\n",
    "            self.seg_func = tf.segment_sum\n",
    "        elif seg_func_name == 'mean':\n",
    "            self.seg_func = tf.segment_mean\n",
    "        elif seg_func_name == 'max':\n",
    "            self.seg_func = tf.segment_max\n",
    "        else:\n",
    "            self.seg_func = tf.segment_sum\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(SegmentLayer, self).build(input_shape)\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return K.cast(K.not_equal(x[1], -1), 'bool')\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        rnn_inp = x[0]\n",
    "        word_end_idx = x[1]\n",
    "        segment_mask = x[2]\n",
    "\n",
    "        # Need the max doc len\n",
    "        max_word_len = tf.shape(word_end_idx)[1]\n",
    "        \n",
    "        def f(inp):\n",
    "            '''\n",
    "            Performs a segmented sum on each input of (max_char_len, char_lstm_dim)\n",
    "            '''\n",
    "            mat = inp[0]\n",
    "            seg_mask = inp[1]\n",
    "            \n",
    "            # perform the segmented sum along the first axis\n",
    "            seg_sum = self.seg_func(mat, seg_mask)[:-1]  # don't want the last segment\n",
    "            \n",
    "            # need to pad the result such that we always have vectors of max_token_len\n",
    "            seg_sum_shape = tf.shape(seg_sum)\n",
    "            zero_pad = tf.zeros((max_word_len - seg_sum_shape[0], seg_sum_shape[1]), dtype='float32')\n",
    "            seg_sum_padded = tf.concat(0, [seg_sum, zero_pad])\n",
    "            \n",
    "            return seg_sum_padded\n",
    "        \n",
    "        map_fn_out = tf.map_fn(f, elems=(rnn_inp, segment_mask), dtype='float32')\n",
    "        \n",
    "        return map_fn_out\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        rnn_shape = input_shape[0]\n",
    "        ind_shape = input_shape[1]\n",
    "        \n",
    "        return (rnn_shape[0], ind_shape[1], rnn_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a method for setting the models word_embedding layer to have pretrained embeddings\n",
    "def set_embeddings(embedding, weights, token_map):\n",
    "    '''\n",
    "    Takes in a gensim Word2Vec model and our keras model and then adapts the word_embeddings\n",
    "    in our model to the pre-trained vectors from the w2v model if they exist. Otherwise, they are\n",
    "    left to the original initalization\n",
    "    \n",
    "    Paramters\n",
    "    =========\n",
    "    w2v_model : gensim.models.word2vec.Word2Vec\n",
    "        Word2Vec model that is already loaded with pre-trained embedings\n",
    "    keras_model : keras.engine.training.Model\n",
    "        Keras model that has been constructed such that the word embedding layer has the same\n",
    "        number of embedding dimensions as the pre-trained embeddings\n",
    "    token_map : dict\n",
    "        map from token to index in the vocab\n",
    "    '''    \n",
    "    for token, ind in token_map.iteritems():\n",
    "        try:\n",
    "            pre_trained_emb = w2v_model[token]     \n",
    "        except:\n",
    "            pre_trained_emb = weights[ind]\n",
    "        weights[ind] = pre_trained_emb\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_model(word_vocab_size, char_vocab_size,\n",
    "                    w_emb_dim=100, w_lstm_dim=128, \n",
    "                    c_emb_dim=100, c_lstm_dim=128, \n",
    "                    embedding=None, token2idx=None, trainable_embedding=True, dropout=0.5, \n",
    "                    char_emb=True):\n",
    "    '''\n",
    "    Constructs the NERNN in keras / tensorflow\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    word_vocab_size : int\n",
    "        size of the token vocabulary\n",
    "    char_vocab_size : int\n",
    "        size of the character vocabulary\n",
    "    w_emb_dim : int\n",
    "        size of token lstm layer\n",
    "    c_emb_dim : int\n",
    "        size of the char lstm layer\n",
    "    embedding : optional, gensim.model.word2vec.Word2Vec\n",
    "        pre-trained embeddings passed in the form of a gensim model\n",
    "    token2idx : optional, dict\n",
    "        map from tokens to indices\n",
    "    trainable_embedding : bool\n",
    "        indicates whether word embeddings are trainable\n",
    "    chat_emb : bool\n",
    "        indicates whether to use character embeddings\n",
    "    '''\n",
    "    \n",
    "    # =========================== #\n",
    "    # 1. Construct word Embedding #\n",
    "    # =========================== #\n",
    "\n",
    "    word_model = Sequential()\n",
    "    \n",
    "    if embedding:\n",
    "        \n",
    "        # Make some assertions\n",
    "        assert token2idx is not None, \"Must pass a map from token to idx for current training set with an embedding\"\n",
    "        assert w_emb_dim == embedding.vector_size, \"Word embedding dimension must be same as passed embedding\"\n",
    "        \n",
    "        # Set the weights to the pre-trained weights from the passed embedding\n",
    "        weights = np.random.uniform(-0.05, 0.05, size=(word_vocab_size+2, w_emb_dim))\n",
    "        weights = set_embeddings(embedding, weights, token2idx)\n",
    "        \n",
    "        # Construct the embedding layer\n",
    "        word_emb = Embedding(input_dim=word_vocab_size+2, output_dim=w_emb_dim, \n",
    "                             input_length=None, mask_zero=True, weights=weights,\n",
    "                             trainable=trainable_embedding, name='word_embedding')\n",
    "        \n",
    "    else:\n",
    "        # Otherwise randomly initialize weights\n",
    "        word_emb = Embedding(input_dim=word_vocab_size+2, output_dim=w_emb_dim,\n",
    "                            input_length=None, mask_zero=True, \n",
    "                             trainable=trainable_embedding, name='word_embedding')\n",
    "    word_model.add(word_emb)\n",
    "\n",
    "#     word_model.add(Bidirectional(GRU(w_lstm_dim, return_sequences=True, dropout_W=0.5)))\n",
    "\n",
    "    # ============================== #\n",
    "    # 2. Construct the character RNN #\n",
    "    # ============================== #\n",
    "\n",
    "    if char_emb:\n",
    "        char_model = Sequential()\n",
    "        char_model.add(Embedding(input_dim=char_vocab_size+2, output_dim=c_emb_dim, \n",
    "                                 input_length=None, mask_zero=True, name='char_embedding'))\n",
    "        char_model.add(Bidirectional(GRU(c_lstm_dim, return_sequences=True, dropout_W=dropout)))\n",
    "        temp_out = char_model.output\n",
    "\n",
    "    # ====================================== #\n",
    "    # 3. Merge the Word RNN and the Char RNN #\n",
    "    # ====================================== #\n",
    "\n",
    "    # If we want to use character embeddings then extract the character representations from the\n",
    "    # character RNN\n",
    "    if char_emb:\n",
    "        # Create an input for the matrix of word end indices\n",
    "        inds = Input(shape=(None,), dtype='int64')\n",
    "        \n",
    "        # Create an input for the matrix of segmentation masks\n",
    "        seg_mask = Input(shape=(None,), dtype='int64')\n",
    "\n",
    "        # Slice the character model out\n",
    "#         char_model_slice = GatherLayer()([temp_out, inds])\n",
    "        \n",
    "        # Segment the character model\n",
    "        char_model_seg = SegmentLayer()([temp_out, inds, seg_mask])\n",
    "        \n",
    "#         model = Model([word_model.input, char_model.input, inds, seg_mask], output=[char_model_seg])\n",
    "#         return model\n",
    "\n",
    "        # Concatenate the outputs of the word model and the sliced character model\n",
    "        merge_out = merge([word_model.output, char_model_seg], mode='concat', concat_axis=2)\n",
    "    else:\n",
    "        merge_out = word_model.output\n",
    "\n",
    "    # Add Bidirectional lstm here\n",
    "    gru_out = Bidirectional(GRU(w_emb_dim, return_sequences=True, dropout_W=dropout))(merge_out)\n",
    "    \n",
    "    # ================== #\n",
    "    # 4. Compute Output  #\n",
    "    # ================== #\n",
    "\n",
    "    # Time distribute a final layer for binary output\n",
    "    fout = TimeDistributed(Dense(1, activation='sigmoid'))(gru_out)\n",
    "    \n",
    "    model = Model([word_model.input, char_model.input, inds, seg_mask], output=[fout])\n",
    "    \n",
    "    # ====================================== #\n",
    "    # 5. Define Accuracy Metrics and Compile #\n",
    "    # ====================================== #\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        den = K.sum(K.cast(K.not_equal(y_true, -1), dtype='float32'))\n",
    "        num = K.sum(K.cast(K.equal(y_true, K.round(y_pred)) & K.not_equal(y_true, -1), dtype='float32'))\n",
    "        return num / den\n",
    "    \n",
    "    def true_pos(y_true, y_pred):\n",
    "        den = tf.reduce_sum(tf.cast(tf.equal(tf.round(y_pred), 1) & tf.not_equal(y_true, -1), dtype='float32'))\n",
    "        \n",
    "        i = tf.equal(y_true, tf.round(y_pred)) & \\\n",
    "                tf.equal(1., tf.round(y_pred)) & \\\n",
    "                tf.not_equal(y_true, -1)\n",
    "                \n",
    "        num = tf.reduce_sum(tf.cast(i, dtype='float32'))\n",
    "\n",
    "        frac = tf.select(den==0., 0., num / den)\n",
    "        \n",
    "        return frac\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[accuracy, true_pos])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Construct a gensim model that we can use to access underlying embeddings from the massive GoogleNews embedding matrix\n",
    "- 3 Million words in GoogleNews Vocab with 300 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the gensim word2vec model\n",
    "w2vmodel = Word2Vec.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Bring in Wikipedia Data to Train On\n",
    "- Downloaded WikiNER data into data directory\n",
    "- Will read in pre-split train/test/dev data into pandas\n",
    "- Preprocess the data to only tag PER (peoples names)\n",
    "- Will batch sentences together into 'Documents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "def prepend_zeros(arr, num_zeros=1, dtype='int64'):\n",
    "    '''\n",
    "    Takes a 1-D numpy array and prepends the specified number of zeros\n",
    "    '''\n",
    "    zs = np.zeros(shape=(num_zeros,), dtype=dtype)\n",
    "    return np.concatenate([zs, arr], axis=0)\n",
    "\n",
    "def spans(txt, tokens):\n",
    "    '''\n",
    "    Takes the original (read: \"untokenized\" text) and the tokens and returns a list of word\n",
    "    end indices.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    txt : string\n",
    "        untokenized / raw string we want to index the tokens into\n",
    "    tokens : list, array\n",
    "        list of tokens that make up the txt\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    word_inds : list\n",
    "        list of word span indices for the tokens into the txt\n",
    "    '''\n",
    "    word_inds = []\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        word_inds.append((offset, offset+len(token)))\n",
    "        offset += len(token)\n",
    "    return word_inds\n",
    "\n",
    "def segmentation_mask(word_end_inds):\n",
    "    '''\n",
    "    Takes the start and end (inclusive) index of the tokens and returns a mask with length num of characters\n",
    "    that masks each token with a monotonically increasing index\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    ['H','e','l','l','o',' ','W','o','r','l','d'] -> [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "    '''\n",
    "    doc_len = max(word_end_inds) + 1\n",
    "    \n",
    "    word_inds = [-1] + word_end_inds\n",
    "    \n",
    "    seg_mask = np.zeros(doc_len, dtype='int64')\n",
    "    \n",
    "    for i in range(len(word_inds) - 1):\n",
    "        s = word_inds[i]\n",
    "        e = word_inds[i+1]\n",
    "        seg_mask[s+1:e+1] = i\n",
    "    return seg_mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Methods for Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==== #\n",
    "# Vars #\n",
    "# ==== #\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = './data'\n",
    "WIKI_DATA = os.path.join(DATA_DIR, 'WikiGold')\n",
    "\n",
    "# Misc.\n",
    "DOCSTART_TAG = '-DOCSTART-'\n",
    "\n",
    "# ======================================= #\n",
    "# Methods for Reading and Processing Data #\n",
    "# ======================================= #\n",
    "\n",
    "# Read in the data only (no processing except putting in in pandas)\n",
    "def read_wiki_datasets(data_dir):\n",
    "    '''\n",
    "    Reads in the wikipedia datasets from the data directory\n",
    "    \n",
    "    Paramters\n",
    "    =========\n",
    "    data_dir : str\n",
    "        path to the data directory\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    train_df : pandas.core.frame.DataFrame\n",
    "        dataframe with training data\n",
    "    test_df : pandas.core.frame.DataFrame\n",
    "        dataframe with test data\n",
    "    dev_df : pandas.core.frame.DataFrame\n",
    "        dataframe with dev data\n",
    "    TODO: (areiner) what the hell is dev data?... I didn't do these splits\n",
    "    '''\n",
    "    \n",
    "    dataset_names = ('train', 'test', 'dev')\n",
    "    datasets = []  # ordered train, test, dev\n",
    "    for dname in dataset_names: \n",
    "        found_datasets = glob.glob(data_dir + '/*' + dname + '*.pkl')\n",
    "        if len(found_datasets)==0:\n",
    "            print \"No dataset with name {} found\".format(dname)\n",
    "        elif len(found_datasets) > 1:\n",
    "            print \"Multiple dataset with name {} found\".format(dname)\n",
    "        else:\n",
    "            df = pd.read_pickle(found_datasets[0])\n",
    "            datasets.append(df)\n",
    "    return datasets\n",
    "\n",
    "# Method to filter all tags such that it is only 'O' or 'PER'\n",
    "def modify_wikigold_tags(df):\n",
    "    '''\n",
    "    Takes in a dataframe with columns \"sentence\" and \"tags\" and modifies\n",
    "    the \"tags\" column to convert everything that is not \"B-PER\" to \"O\" and\n",
    "    converts \"B-PER\" to \"PER\"\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        input dataframe of sentences and tags (with columns \"sentence\" and \"tags\")\n",
    "    '''\n",
    "    def modify_taglist(taglist):\n",
    "        '''\n",
    "        Function to apply to each array of tags in each cell of the \"tags\"\n",
    "        column in the dataframe\n",
    "        '''\n",
    "        m = {'B-PER': 'PER'}\n",
    "        return map(lambda v: m.get(v, 'O'), taglist)\n",
    "\n",
    "    df['tags'] = df['tags'].apply(modify_taglist)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Method to find the index for document separations\n",
    "def calc_docstart_inds(df):\n",
    "    '''\n",
    "    Takes in a dataframe and looks for specific document start tags\n",
    "    '''\n",
    "    return df.index[df.sentence.map(lambda x: DOCSTART_TAG in x)].get_values()\n",
    "\n",
    "# Method to concatenate sentence tags to document tags\n",
    "def construct_doc_tag(dfs, max_sent_per_doc=None):\n",
    "    '''\n",
    "    Takes a DataFrame slice and returns a dataframe of tokens and tags that\n",
    "    concatenates all sentence tokens and tags for the whole dataframe, potentially\n",
    "    into groups of size specified by max_sent_per_doc size\n",
    "    '''\n",
    "    new_df = pd.DataFrame(columns=['document', 'tags'])\n",
    "    \n",
    "    sentences = []\n",
    "    tags = []\n",
    "    \n",
    "    for i, (_, row) in enumerate(dfs.iterrows()):\n",
    "        \n",
    "        if (max_sent_per_doc is not None) and i!=0 and (i % max_sent_per_doc == 0):\n",
    "            new_df = new_df.append({'document': sentences, 'tags': tags}, ignore_index=True)\n",
    "            sentences = []\n",
    "            tags = []\n",
    "\n",
    "        tokens = row.sentence\n",
    "        tagseq = row.tags\n",
    "        \n",
    "        sentences.extend(tokens)\n",
    "        tags.extend(tagseq)\n",
    "    \n",
    "    new_df = new_df.append({'document': sentences, 'tags': tags}, ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Method to cluster sentence token dataframes into documents\n",
    "def transform_sent_to_docs(df, max_sent_per_doc=None):\n",
    "    '''\n",
    "    Transform a DataFrame of lists of tokens and tags per sentence into\n",
    "    a DataFrame of \"Documents\" and the tags for that document.\n",
    "    At a minimum, we split by document length as per the wikipedia page\n",
    "    '''\n",
    "    doc_df = pd.DataFrame(columns=['document', 'tags'])\n",
    "    \n",
    "    # 1. Calculate the indices of Document starts\n",
    "    doc_starts = prepend_zeros(calc_docstart_inds(df))\n",
    "    \n",
    "    # 2. Slice the df for the sentences in each document\n",
    "    for i, (start, end) in enumerate(zip(doc_starts[:-1], doc_starts[1:])):\n",
    "        if i != 0:\n",
    "            start += 1\n",
    "        dfs = df.iloc[start:end]\n",
    "        inc_doc_df = construct_doc_tag(dfs, max_sent_per_doc=max_sent_per_doc)\n",
    "        doc_df = doc_df.append(inc_doc_df)\n",
    "    \n",
    "    doc_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return doc_df\n",
    "        \n",
    "# Method that adds a column which contains the untokenized sentence / document\n",
    "def untokenize_column(df, col='document', new_col='document_string'):\n",
    "    '''\n",
    "    Untokenizes a column of lists of tokens. The intended use in the pipeline is to\n",
    "    construct untokenized strings after clustering sentences into documents or partial\n",
    "    documents.\n",
    "    '''\n",
    "    df[new_col] = df[col].map(untokenize)\n",
    "    return df\n",
    "\n",
    "# Method that adds a column which contains the index of the tokens into the untokenized sentence / document\n",
    "def word_index_columns(df, doc_col='document_string', tok_col='document'):\n",
    "    '''\n",
    "    Takes a column of strings (either sentences or documents as long as it's one continuous string) \n",
    "    and creats two new columns a 'word_start_inds' column that contains the start indices of all tokens\n",
    "    and a 'word_end_inds' that contains the end indices of all tokens\n",
    "    '''\n",
    "    # Calculate the word spanning indices\n",
    "    word_inds = df.apply(lambda r: spans(r[doc_col], r[tok_col]), axis=1)\n",
    "    word_start_inds = word_inds.apply(lambda v: [e[0] for e in v])\n",
    "    word_end_inds = word_inds.apply(lambda v: [e[1] - 1 for e in v])\n",
    "    word_segment_mask = word_end_inds.map(segmentation_mask)\n",
    "    \n",
    "    # Insert new columns\n",
    "    df['word_start_inds'] = word_start_inds\n",
    "    df['word_end_inds'] = word_end_inds\n",
    "    df['word_segment_mask'] = word_segment_mask\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Methods for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct word and character maps\n",
    "def construct_map(element_lists, vocab_size=None):\n",
    "    '''\n",
    "    Constructs a vocabulary from \n",
    "    '''\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for els in element_lists:\n",
    "        c.update(els)\n",
    "\n",
    "    if vocab_size is not None:\n",
    "        most_common = [x[0] for x in c.most_common(vocab_size)]\n",
    "        hash_map = dict(zip(most_common, range(1, len(most_common)+1)))\n",
    "    else:\n",
    "        hash_map = dict(zip(c.keys(), range(1, len(c)+1)))\n",
    "\n",
    "    return hash_map\n",
    "\n",
    "def reverse_map(m):\n",
    "    m_inv = dict(((ind, k) for k, v in m.iteritems()))\n",
    "    return m_inv\n",
    "    \n",
    "def character_column(df):\n",
    "    '''\n",
    "    Inserts a column of the individual characters into the dataframe \n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        dataframe containing the string / untokenized documents\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    new_df : pandas.core.frame.DataFrame\n",
    "        Updated df\n",
    "    '''\n",
    "    df['chars'] = df['document_string'].map(lambda v: list(v))\n",
    "    return df\n",
    "\n",
    "def construct_word_char_maps(df, vocab_size=None, return_inv_dicts=False):\n",
    "    '''\n",
    "    Construct the word and character maps from the dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    df : pandas DataFrame\n",
    "        dataframe with tokens and character lists already constructed\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    token_map : dict\n",
    "    char_map : dict\n",
    "    '''\n",
    "    token_map = construct_map(df['document'])\n",
    "    char_map = construct_map(df['chars'])\n",
    "    \n",
    "    if return_inv_dicts:\n",
    "        token_map_inv = reverse_map(token_map)\n",
    "        char_map_inv = reverse_map(char_map)\n",
    "        \n",
    "        return token_map, char_map, token_map_inv, char_map_inv\n",
    "    else:\n",
    "        return token_map, char_map\n",
    "\n",
    "def encode_strings(element_lists, hash_map):\n",
    "    '''\n",
    "    Encode the element_list in terms of integers\n",
    "    NOTE: 0 is reserved for masking\n",
    "    '''\n",
    "    new_element_list = []\n",
    "    for els in element_lists:\n",
    "        new_els = map(lambda x: hash_map.get(x, len(hash_map)+1), els)\n",
    "        new_element_list.append(new_els)\n",
    "    return new_element_list\n",
    "    \n",
    "def encode_tokens_chars(df, token_map, char_map):\n",
    "    '''\n",
    "    Encode the tokens and characters\n",
    "    '''\n",
    "    df['token_enc'] = encode_strings(df['document'], token_map)\n",
    "    df['char_enc'] = encode_strings(df['chars'], char_map)\n",
    "    return df\n",
    "\n",
    "def _encode_tags(tag_list):\n",
    "    '''\n",
    "    Encode a taglist\n",
    "    '''\n",
    "    def _tag_str_to_int(tag_str):\n",
    "        if tag_str == 'PER':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    tags_enc = map(_tag_str_to_int, tag_list)\n",
    "    return tags_enc\n",
    "    \n",
    "def encode_tags(df):\n",
    "    '''\n",
    "    Encode the tags as binary outcomes\n",
    "    \n",
    "    Parameters\n",
    "    =========\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        dataframe with character tags that are in the set {'O', and 'PER'}\n",
    "    '''\n",
    "    \n",
    "    df['tags_enc'] = df['tags'].apply(_encode_tags)\n",
    "    return df\n",
    "\n",
    "def _construct_character_tags(word_start_inds, word_end_inds, char_list, tag_enc):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    char_tags = np.zeros(len(char_list), dtype='int64')\n",
    "    \n",
    "    word_start_inds = np.array(word_start_inds)\n",
    "    word_end_inds = np.array(word_end_inds)\n",
    "    tag_enc = np.array(tag_enc)\n",
    "    \n",
    "    word_start_ind_tag = word_start_inds[tag_enc==1]\n",
    "    word_end_ind_tag = word_end_inds[tag_enc==1]\n",
    "    word_span_tag = zip(word_start_ind_tag, word_end_ind_tag)\n",
    "    \n",
    "    for (s, e) in word_span_tag:\n",
    "        char_tags[s:e+1] = 1\n",
    "    \n",
    "    return list(char_tags)\n",
    "\n",
    "def construct_character_tags(df):\n",
    "    df['char_tag_mask'] = df.apply(lambda r: _construct_character_tags(r.word_start_inds, r.word_end_inds, \n",
    "                                                                       r.chars, r.tags_enc), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Full data processing pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_wikigold_dataset(df, max_sent_per_doc=None):\n",
    "    \n",
    "    # Text Processing\n",
    "    df = modify_wikigold_tags(df)\n",
    "    df = transform_sent_to_docs(df, max_sent_per_doc=max_sent_per_doc) # Concatenate sentence tokens into topics\n",
    "    df = untokenize_column(df)  # (approximately) concatenate the tokens into documents\n",
    "    df = word_index_columns(df)  # index the tokens into the untokenized strings\n",
    "    df = character_column(df)\n",
    "\n",
    "    # Encoding work\n",
    "    tm, cm = construct_word_char_maps(df)  # first construct the hash maps\n",
    "    df = encode_tokens_chars(df, tm, cm)  # encode the tokens and the characters\n",
    "    df = encode_tags(df)  # encode the tags as binary outcomes 'O' -> 0 and 'PER' -> 1\n",
    "    \n",
    "    # Extra info for model validation\n",
    "    df = construct_character_tags(df)\n",
    "    \n",
    "    return df, tm, cm\n",
    "\n",
    "def run_wikigold_data_pipeline(data_path, max_sent_per_doc=None):\n",
    "    '''\n",
    "    Runs the entire transformation pipeline for WikiGold data.\n",
    "    \n",
    "    Pipeline Steps\n",
    "    --------------\n",
    "    1. Read in wiki datasets (train, test, dev)\n",
    "    2. Modify the IOB tags to only 'O' and 'PER'\n",
    "    For each dataset in the wiki datasets:\n",
    "        3. Take sentence examples and concatenate them into individual documents\n",
    "        4. Take the documents which consist of lists of tokens and \"untokenize\" them into continuous strings\n",
    "        5. Take the tokens and index them into the untokenized text so we have start and end indices for each token\n",
    "        6. Split up the untokenized text into a list of characters\n",
    "        7. Construct element -> monotonically increasing index map for both tokens and characters\n",
    "        8. Encode the tokens and characters by their index in their respective maps\n",
    "        9. Encode the tags as binary outcomes 'O' -> 0 and 'PER' -> 1\n",
    "    '''\n",
    "    # 1. Reading in data\n",
    "    wiki_datasets = read_wiki_datasets(WIKI_DATA)\n",
    "\n",
    "    wiki_datasets_processed = []\n",
    "    wiki_datasets_maps = []\n",
    "    # 2. Run processing on each dataframe\n",
    "    for df in wiki_datasets:\n",
    "        new_df, token_map, char_map = process_wikigold_dataset(df, max_sent_per_doc=max_sent_per_doc)\n",
    "        wiki_datasets_processed.append(new_df)\n",
    "        wiki_datasets_maps.append((token_map, char_map))\n",
    "    \n",
    "    return wiki_datasets_processed, wiki_datasets_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train, test, dev), ((tm_train, cm_train), (tm_test, cm_test), (tm_dev, cm_dev)) = \\\n",
    "                                run_wikigold_data_pipeline(WIKI_DATA, max_sent_per_doc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Final padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_encoding_column(col, value=0):\n",
    "    '''\n",
    "    Takes a pandas series of lists of encodings and returns a single matrix with padded results\n",
    "    '''    \n",
    "    col_padded = sequence.pad_sequences(col.tolist(), padding='post', value=value)\n",
    "    return col_padded\n",
    "\n",
    "def generate_padded_data(df):\n",
    "    '''\n",
    "    Take the dataframe and return numpy matrices with padded encodings\n",
    "    '''\n",
    "    cols_to_pad = ['token_enc', 'char_enc', 'tags_enc', \n",
    "                   'word_start_inds', 'word_end_inds', 'word_segment_mask']\n",
    "    padded_data = []\n",
    "    max_doc_len = max(df.document.map(len))\n",
    "\n",
    "    for col in cols_to_pad:\n",
    "        if ('tag' in col) or ('inds' in col):\n",
    "            col_pad = pad_encoding_column(df[col], value=-1)\n",
    "        elif 'segment' in col:\n",
    "            col_pad = segment_mask_padding2(df[col])\n",
    "        else:\n",
    "            col_pad = pad_encoding_column(df[col])\n",
    "        padded_data.append(col_pad)\n",
    "    return padded_data\n",
    "\n",
    "def segment_mask_padding(seg_col, max_doc_len):\n",
    "    col_pad = pad_encoding_column(seg_col, value=-1)\n",
    "    \n",
    "    max_char_len = col_pad.shape[1]\n",
    "    \n",
    "    for i, r in enumerate(col_pad):\n",
    "        last_word_idx = max(r)\n",
    "        idx_last_word = np.argmax(r)\n",
    "        mask_inds = (r == -1)\n",
    "        \n",
    "        # from idx of last mask + 1 put enough unique values so there are as many unique values as \n",
    "        # the maximum number of unique tokens\n",
    "        total_pad = np.zeros(mask_inds.sum(), dtype='int64')\n",
    "        fake_unique = np.arange(last_word_idx + 1, max_doc_len)\n",
    "        total_pad[:len(fake_unique)] = fake_unique\n",
    "        \n",
    "        # then fill in the remaing open slots with the last value\n",
    "        total_pad[len(fake_unique):] = fake_unique[:][-1:]\n",
    "        r[mask_inds] = total_pad\n",
    "    return col_pad\n",
    "\n",
    "def segment_mask_padding2(seg_col):\n",
    "    col_pad = pad_encoding_column(seg_col, value=-1)\n",
    "    \n",
    "    for r in col_pad:\n",
    "        last_word_idx = max(r)\n",
    "        mask_inds = (r == -1)\n",
    "        \n",
    "        r[mask_inds] = last_word_idx + 1\n",
    "    return col_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Construct Model and Bring in Pre-trained word emebddings\n",
    "- For now, we will use GoogleNews embedding vectors with dimensionality 300\n",
    "- We will not pre-train character emebeddings for now\n",
    "    - Why? bc I haven't found character embedings yet and I would imagine the best representations can different markedly from use case to use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model construction\n",
    "word_vocab_size = len(tm_train)\n",
    "char_vocab_size = len(cm_train)\n",
    "model = construct_model(word_vocab_size=word_vocab_size, char_vocab_size=char_vocab_size,\n",
    "                        w_emb_dim=30, w_lstm_dim=16, c_emb_dim=15, c_lstm_dim=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Train the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "token_enc_train, char_enc_train, tags_enc_train, word_start_ind_train, word_end_ind_train, word_segment_mask_train = \\\n",
    "                                                                                        generate_padded_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "288/288 [==============================] - 745s - loss: 0.2146 - accuracy: 0.9600 - true_pos: nan   \n",
      "Epoch 2/10\n",
      "102/288 [=========>....................] - ETA: 482s - loss: 0.1390 - accuracy: 0.9646 - true_pos: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-0f97f9d5f666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit([token_enc_train, char_enc_train, word_end_ind_train, word_segment_mask_train], \n\u001b[0;32m----> 2\u001b[0;31m           np.expand_dims(tags_enc_train, 2), batch_size=2)\n\u001b[0m",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([token_enc_train, char_enc_train, word_end_ind_train, word_segment_mask_train], \n",
    "          np.expand_dims(tags_enc_train, 2), batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Methods for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masks**\n",
    "- Tokens -> 0\n",
    "- Characters -> 0\n",
    "- Tags -> -1\n",
    "- Word Indices -> -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_char_tags(word_start_inds, word_end_inds, predicted_tag_enc):\n",
    "    '''\n",
    "    Returns lists the same length as the number of characters in the document string\n",
    "    that encodes characters as 1 when they belong to a tagged entity\n",
    "    \n",
    "    Paramters\n",
    "    =========\n",
    "    word_start_inds : numpy array\n",
    "        The padded word start indices for a single example\n",
    "        \n",
    "    word_end_inds : numpy array\n",
    "        The padded word end indices for a single example\n",
    "    \n",
    "    predicted_tag_enc : numpy array\n",
    "        The predicted token-wise tag encoding for a single example with padded (nonsense) predictions left in\n",
    "        Note: this is the form of the output from the model\n",
    "    '''\n",
    "    # 1. Get the length of the document string (num of characters in example) from the word indices\n",
    "    filt = word_end_inds!=-1\n",
    "    word_start_inds = word_start_inds[filt]\n",
    "    word_end_inds = word_end_inds[filt]\n",
    "    doc_string_len = word_end_inds.max() + 1\n",
    "    \n",
    "    # 2. Remove the padded tags\n",
    "    predicted_tag_enc = predicted_tag_enc[filt]\n",
    "\n",
    "    # 3. Fill in a numpy array of tags the length of the unpadded document string\n",
    "    char_tags = np.zeros(doc_string_len, dtype='int64')\n",
    "    \n",
    "    word_start_ind_tag = word_start_inds[predicted_tag_enc==1]\n",
    "    word_end_ind_tag = word_end_inds[predicted_tag_enc==1]\n",
    "    word_span_tag = zip(word_start_ind_tag, word_end_ind_tag)\n",
    "    \n",
    "    for (s, e) in word_span_tag:\n",
    "        char_tags[s:e+1] = 1\n",
    "    \n",
    "    return list(char_tags)\n",
    "    \n",
    "def tagged_entities_in_docstring(char_lists,\n",
    "                                 word_start_inds_arr, word_end_inds_arr,\n",
    "                                 yhats,\n",
    "                                 true_char_tag_enc_lists,\n",
    "                                 thresh=0.5):\n",
    "    '''\n",
    "    Takes the model output and returns:\n",
    "    (1) the array of characters\n",
    "    (2) the characters that actually belong to tags\n",
    "    (3) the characters we estimate belong to tags\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    char_lists : nested list\n",
    "        nested list of character lists for the examples passed to produce the provided\n",
    "        yhat model output\n",
    "        \n",
    "    word_star_inds_arr : numpy array\n",
    "        padded array of word start indices\n",
    "    \n",
    "    word_end_inds_arr : numpy array\n",
    "        padded array of word end indices\n",
    "        \n",
    "    yhats : numpy array\n",
    "        output of the model\n",
    "        \n",
    "    true_char_tag_enc_lists : nested list\n",
    "        true char by char tag encoding of the model\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    res : list of 3-tuple\n",
    "        each tuple corresponds to a single training example / document / partial document\n",
    "        and is composed of\n",
    "        (i) sequence of characters that make up the document \n",
    "        (ii) sequence of predicted tags for each character representing a mask of what tokens were tagged\n",
    "        (iii) sequence of true tags for each character representing a mask of that tokens were tagged\n",
    "    \n",
    "    NOTE\n",
    "    ----\n",
    "    All the inputs should share the same size in the 0th dimension.\n",
    "    For dim 1, the sizes should be:\n",
    "        (len of doc i in characters, \n",
    "        max document len in tokens,\n",
    "        max document len in tokens,\n",
    "        max document len in tokens, \n",
    "        len of doc i in characters)\n",
    "    '''\n",
    "    res = []\n",
    "    for i in range(len(word_start_inds_arr)):\n",
    "        \n",
    "        # 1. Get each example passed\n",
    "        \n",
    "        # Get the predictions\n",
    "        char_list = char_lists[i]\n",
    "        word_start_inds = word_start_inds_arr[i]\n",
    "        word_end_inds = word_end_inds_arr[i]\n",
    "        yhat = yhats[i]\n",
    "        predicted_tag_enc =(yhat >= thresh).astype('int64')\n",
    "        \n",
    "        # Extract the true character tags\n",
    "        true_char_tag_enc = true_char_tag_enc_lists[i]\n",
    "        \n",
    "        # Calculate the predicted char_tags\n",
    "        predicted_char_tags = predict_char_tags(word_start_inds, word_end_inds, predicted_tag_enc)\n",
    "        \n",
    "        # Append the example results\n",
    "        res.append((np.array(char_list), np.array(predicted_char_tags), np.array(true_char_tag_enc)))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def add_tagged_char_mask_to_df(model, df, batch_size=5, thresh=0.5):\n",
    "    \n",
    "    token_enc, char_enc, tags_enc, word_start_ind, word_end_ind = generate_padded_data(df)\n",
    "    x = [token_enc, char_enc, word_end_ind]\n",
    "    if 'prob' in df.columns:\n",
    "        y_preds = df.prob.tolist()\n",
    "    else:\n",
    "        y_preds = model.predict(x, batch_size=batch_size)[..., 0]\n",
    "        \n",
    "    char_lists = df.chars.tolist()\n",
    "    true_char_tag_enc_lists = df.char_tag_mask.tolist()\n",
    "    \n",
    "    res_list = tagged_entities_in_docstring(char_lists,\n",
    "                                                 word_start_ind, word_end_ind, \n",
    "                                                 y_preds,\n",
    "                                                 true_char_tag_enc_lists, \n",
    "                                                 thresh=thresh)\n",
    "    pred_char_tag = [r[1] for r in res_list]\n",
    "    df['pred_char_tag_mask'] = pred_char_tag\n",
    "    return df\n",
    "\n",
    "def add_predictions_to_df(x, model, df, batch_size=5, thresh=0.5):\n",
    "    if 'prob' in df.columns:\n",
    "        y_preds = df.prob.tolist()\n",
    "    else:\n",
    "        y_preds = model.predict(x, batch_size=batch_size)[..., 0]\n",
    "    y_preds_nopad = []\n",
    "    yhats = []\n",
    "    for i in range(len(y_preds)):\n",
    "        token_mask = x[0][i] != 0\n",
    "        y_pred_nopad = y_preds[i][token_mask]\n",
    "        y_preds_nopad.append(y_pred_nopad)\n",
    "        yhats.append((y_pred_nopad > thresh).astype('int32'))\n",
    "    df['prob'] = y_preds_nopad\n",
    "    df['yhat'] = yhats\n",
    "    return df\n",
    "\n",
    "def calculate_and_add_preds_to_df(model, df, batch_size=5, thresh=0.5):\n",
    "    '''\n",
    "    Calculate and add preds\n",
    "    '''\n",
    "    token_enc, char_enc, tags_enc, word_start_ind, word_end_ind = generate_padded_data(df)\n",
    "    x = [token_enc, char_enc, word_end_ind]\n",
    "    df = add_predictions_to_df(x, model, df, batch_size=batch_size)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Run Predictions on the train and test dataframes and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_p = calculate_and_add_preds_to_df(model, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_p = add_tagged_char_mask_to_df(model, train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'document', u'tags', u'document_string', u'word_start_inds',\n",
       "       u'word_end_inds', u'chars', u'token_enc', u'char_enc', u'tags_enc',\n",
       "       u'char_tag_mask', u'prob', u'yhat', u'pred_char_tag_mask'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000111111110000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011111110110111111100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "print ''.join(map(str,train_p.char_tag_mask[i]))\n",
    "print\n",
    "print ''.join(map(str,train_p.pred_char_tag_mask[i]))\n",
    "print\n",
    "print train_p.yhat[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kojima' 'Minoru']\n",
      "[('Kojima', 0.24943741), ('Minoru', 0.27782112)]\n",
      "Lowest scores: ['the' 'the' 'of' 'the' 'was' 'a' ',' 'and' 'and' 'the' 'a' 'of' 'of' 'was'\n",
      " 'was' 'and' ',' ',' 'The' 'and' 'were' '.' 'to' 'and' '.' 'to' 'it' '.'\n",
      " 'when' 'it']\n",
      "Highest scores: ['did' 'Come' 'lyric' '010' 'Punk' 'heavily' 'Minoru' 'synthesized' '.'\n",
      " 'Joke' 'post' 'UK' 'Good' 'Techno' 'Mad' 'punk' 'P.O.P' 'XXX' '010'\n",
      " 'HUMANITY']\n",
      "Highest scores: [ 0.27076656  0.27241933  0.27379623  0.27424937  0.27586141  0.27680361\n",
      "  0.27782112  0.27892116  0.27998835  0.28038624  0.28160602  0.28303012\n",
      "  0.28372702  0.28414869  0.28732777  0.29377851  0.30477554  0.31889755\n",
      "  0.31972563  0.32628825]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "x_in = [token_enc_train[i:i+1], char_enc_train[i:i+1], word_end_ind_train[i:i+1]]\n",
    "\n",
    "out = model.predict(x_in)[0, :, 0]\n",
    "\n",
    "token_ex = np.array(train.document[i])\n",
    "tags_ex = np.array(train.tags[i])\n",
    "tags_enc_ex = np.array(train.tags_enc[i])\n",
    "\n",
    "m = token_ex[(tags_enc_ex.nonzero()[0])]\n",
    "\n",
    "print m\n",
    "print zip(m, out[tags_enc_ex.nonzero()[0]])\n",
    "out_sorted = out.argsort()\n",
    "out_sorted = out_sorted[out_sorted<len(token_ex)]\n",
    "print \"Lowest scores:\", token_ex[out_sorted[:30]]\n",
    "print \"Highest scores:\", token_ex[out_sorted[-20:]]\n",
    "print \"Highest scores:\", out[out_sorted[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "token_enc_test, char_enc_test, tags_enc_test, word_start_ind_test, word_end_ind_test = \\\n",
    "                                                                                        generate_padded_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dorothea' 'Dorothea' 'von' 'Schlegel' 'Rahel' 'Levin' 'Henriette' 'Herz'\n",
      " 'Madame' 'de' 'Sta\\xc3\\xab' 'Friedrich' 'Philipp' 'Moses' 'Mendelssohn'\n",
      " 'Immanuel' 'Kant' 'John' 'Locke' 'Alexander' 'Pope' 'Dorothea']\n",
      "[('Dorothea', 0.00082105485), ('Dorothea', 0.0094426926), ('von', 0.0055673928), ('Schlegel', 0.0051288288), ('Rahel', 0.00068829377), ('Levin', 9.7050888e-06), ('Henriette', 0.00013976262), ('Herz', 2.7702123e-05), ('Madame', 0.010900004), ('de', 0.0044732802), ('Sta\\xc3\\xab', 0.0014611182), ('Friedrich', 0.52245504), ('Philipp', 0.00066411082), ('Moses', 0.00057833287), ('Mendelssohn', 0.0017986018), ('Immanuel', 0.00022611055), ('Kant', 9.4189062e-07), ('John', 7.342115e-05), ('Locke', 7.0163347e-05), ('Alexander', 0.0013022374), ('Pope', 0.051861312), ('Dorothea', 0.01213771)]\n",
      "Lowest scores: ['Kant' 'and' 'and' 'adopted' 'and' 'greatest' 'and' 'critics' 'of'\n",
      " 'translator' 'Levin' ',' 'novelists' 'to' 'musicians' ',' 'convert'\n",
      " 'leading' ',' 'Herz' ',' ',' 'as' ',' 'medieval' 'of' 'which' 'of'\n",
      " 'daughter' 'name']\n",
      "Highest scores: ['who' 'hymns' 'died' 'throughout' '1829' '1839' 'her' 'Friedrich' 'death'\n",
      " 'surrounded' 'moved' ')' '17th' ')' ')' 'Frankfurt' 'surrounded' 'until'\n",
      " 'in' 'in']\n",
      "Highest scores: [ 0.0754  0.076   0.0788  0.0841  0.1002  0.1514  0.3697  0.5225  0.6907\n",
      "  0.7128  0.808   0.8208  0.8746  0.9371  0.9451  0.9731  0.9801  0.9969\n",
      "  0.9973  0.9979]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "x_in = [token_enc_test[i:i+1], char_enc_test[i:i+1], word_end_ind_test[i:i+1]]\n",
    "\n",
    "out = model.predict(x_in)[0, :, 0]\n",
    "\n",
    "token_ex = np.array(test.document[i])\n",
    "tags_ex = np.array(test.tags[i])\n",
    "tags_enc_ex = np.array(test.tags_enc[i])\n",
    "\n",
    "m = token_ex[(tags_enc_ex.nonzero()[0])]\n",
    "\n",
    "print m\n",
    "print zip(m, out[tags_enc_ex.nonzero()[0]])\n",
    "out_sorted = out.argsort()\n",
    "out_sorted = out_sorted[out_sorted<len(token_ex)]\n",
    "print \"Lowest scores:\", token_ex[out_sorted[:30]]\n",
    "print \"Highest scores:\", token_ex[out_sorted[-20:]]\n",
    "print \"Highest scores:\", out[out_sorted[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: How can we determine the end of word indices from the tokens?\n",
    "- we need to use the provided tokens and reconstruct the original string\n",
    "    - We do this via an \"untokenize\" function found off the shelf online\n",
    "    - If we tokenize again (using nltk word_tokenize) do we get the same result?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``\n",
      "\"\n",
      "bonus tracks: \" Battle of\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bonus tracks:\" Battle of One\"( an original song that was also set'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print word_tokenize(res.iloc[4].document_string)[257]\n",
    "print res.iloc[4].document[257]\n",
    "print untokenize(res.iloc[4].document[254:260])\n",
    "\n",
    "def untokenize2(tokens):\n",
    "    import string\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "untokenize2(res.iloc[4].document[254:270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: Why is the true positive metric for the keras model messing up and returning nan?\n",
    "- Extracting the variables from the tensorflow model and constructing the accuracy metric outside works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(dtype='float32')\n",
    "y_pred = model.output\n",
    "\n",
    "def true_pos(y_true, y_pred):\n",
    "    den = tf.reduce_sum(tf.cast(tf.equal(tf.round(y_pred), 1) & tf.not_equal(y_true, -1), dtype='float32'))\n",
    "\n",
    "    i = tf.equal(y_true, tf.round(y_pred)) & \\\n",
    "            tf.equal(1., tf.round(y_pred)) & \\\n",
    "            tf.not_equal(y_true, -1)\n",
    "\n",
    "    num = tf.reduce_sum(tf.cast(i, dtype='float32'))\n",
    "\n",
    "    frac = tf.select(den==0., 0., num / den)\n",
    "\n",
    "    return frac\n",
    "\n",
    "frac = true_pos(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "token_enc_train, char_enc_train, tags_enc_train, word_start_ind_train, word_end_ind_train = \\\n",
    "                                                                                        generate_padded_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = dict(zip(model.input, [token_enc_train[:2,:,],\n",
    "                           char_enc_train[:2, :],\n",
    "                           word_end_ind_train[:2, :]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 26.0\n",
      "0.0 0.0 3.0\n",
      "0.025641 1.0 39.0\n",
      "0.0 0.0 40.0\n",
      "0.047619 1.0 21.0\n",
      "0.0952381 2.0 21.0\n",
      "0.0 0.0 44.0\n",
      "0.0 0.0 42.0\n",
      "0.0 0.0 43.0\n",
      "0.0 0.0 35.0\n",
      "0.037037 2.0 54.0\n",
      "0.0714286 2.0 28.0\n",
      "0.08 2.0 25.0\n",
      "0.03125 1.0 32.0\n",
      "0.0655738 4.0 61.0\n",
      "0.0434783 1.0 23.0\n",
      "0.0714286 2.0 28.0\n",
      "0.0 0.0 27.0\n",
      "0.171429 6.0 35.0\n",
      "0.04 2.0 50.0\n",
      "0.0 0.0 41.0\n",
      "0.030303 1.0 33.0\n",
      "0.0 0.0 48.0\n",
      "0.0 0.0 46.0\n",
      "0.0 0.0 30.0\n",
      "0.0 0.0 33.0\n",
      "0.0 0.0 42.0\n",
      "0.0 0.0 50.0\n",
      "0.0526316 2.0 38.0\n",
      "0.0 0.0 53.0\n",
      "0.0689655 2.0 29.0\n",
      "0.0 0.0 14.0\n",
      "0.0 0.0 53.0\n",
      "0.0 0.0 34.0\n",
      "0.0 0.0 28.0\n",
      "0.0 0.0 14.0\n",
      "0.0 0.0 9.0\n",
      "0.0 0.0 73.0\n",
      "0.0 0.0 68.0\n",
      "0.0454545 4.0 88.0\n",
      "0.0833333 3.0 36.0\n",
      "0.0714286 2.0 28.0\n",
      "0.0 0.0 39.0\n",
      "0.0 0.0 27.0\n",
      "0.0 0.0 31.0\n",
      "0.097561 4.0 41.0\n",
      "0.215686 11.0 51.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-6e1569e3e94c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                      \u001b[0mchar_enc_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                      \u001b[0mword_end_ind_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                                      np.expand_dims(tags_enc_train[i:i+1], 2)]\n\u001b[0m\u001b[1;32m      6\u001b[0m                                          )\n\u001b[1;32m      7\u001b[0m                                      )\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/areiner/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    f, n, d = sess.run([frac, num, den], feed_dict=dict(zip(model.input + [y_true], [token_enc_train[i:i+1], \n",
    "                                                                     char_enc_train[i:i+1],\n",
    "                                                                     word_end_ind_train[i:i+1],\n",
    "                                                                     np.expand_dims(tags_enc_train[i:i+1], 2)]\n",
    "                                         )\n",
    "                                     )\n",
    "            )\n",
    "    \n",
    "\n",
    "    print f, n, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Segmentation Layer that didn't account for padding properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SegmentLayerOld(Layer):\n",
    "    '''\n",
    "    Takes a segmented sum\n",
    "    '''\n",
    "    def __init__(self, seg_func_name='sum', **kwargs):\n",
    "        super(SegmentLayer, self).__init__(**kwargs)\n",
    "        if seg_func_name == 'sum':\n",
    "            self.seg_func = tf.segment_sum\n",
    "        elif seg_func_name == 'mean':\n",
    "            self.seg_func = tf.segment_mean\n",
    "        elif seg_func_name == 'max':\n",
    "            self.seg_func = tf.segment_max\n",
    "        else:\n",
    "            self.seg_func = tf.segment_sum\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(SegmentLayer, self).build(input_shape)\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return K.cast(K.not_equal(x[1], -1), 'bool')\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        rnn_inp = x[0]\n",
    "        segment_mask = x[2]\n",
    "\n",
    "        def f(inp):\n",
    "            '''\n",
    "            Performs a segmented sum on each input of (max_char_len, char_lstm_dim)\n",
    "            '''\n",
    "            mat = inp[0]\n",
    "            seg_mask = inp[1]\n",
    "            seg_sum = self.seg_func(mat, seg_mask)\n",
    "            return seg_sum\n",
    "        \n",
    "        map_fn_out = tf.map_fn(f, elems=(rnn_inp, segment_mask), dtype='float32')\n",
    "        \n",
    "        return map_fn_out\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        rnn_shape = input_shape[0]\n",
    "        ind_shape = input_shape[1]\n",
    "        \n",
    "        return (rnn_shape[0], ind_shape[1], rnn_shape[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_kernel",
   "language": "python",
   "name": "tensorflow_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
